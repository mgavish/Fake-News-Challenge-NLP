{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Deep Learning Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.float_format = '{:, .2f}'.format\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import save, load\n",
    "from numpy import savez_compressed\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#from scipy.misc import comb, logsumexp\n",
    "from sklearn.manifold import TSNE #a tool to visualize high dimensional data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD # dimensionality reduction using truncated SVD (AKA LSA)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.collocations import *\n",
    "import string #python module\n",
    "import re # python regex module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Flatten, MaxPooling1D, Activation, Dropout, LSTM,GlobalMaxPool1D \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import backend\n",
    "# from keras import layers\n",
    "# from keras import models\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75385,)\n",
      "(75385, 836)\n"
     ]
    }
   ],
   "source": [
    "# load pre-processed data\n",
    "\n",
    "from numpy import load\n",
    "target_y = load('model_target_data.npz')\n",
    "target_y = target_y['arr_0']\n",
    "target_y = np.ravel(target_y)\n",
    "print(target_y.shape)\n",
    "\n",
    "features_x =  load('model_data.npz')\n",
    "features_x = features_x['arr_0']\n",
    "print(features_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56538, 836)\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_x, labels, test_size = 0.25, random_state = 1)\n",
    "print(X_train.shape)\n",
    "print(X_train.ndim)\n",
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Sequential keras model and add layers\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(20000, 128))\n",
    "lstm_model.add(LSTM(50, return_sequences=True))\n",
    "lstm_model.add(GlobalMaxPool1D())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(50, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 50)          35800     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 2,598,554\n",
      "Trainable params: 2,598,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56538 samples, validate on 18847 samples\n",
      "Epoch 1/10\n",
      "56538/56538 [==============================] - 1308s 23ms/step - loss: 0.2721 - accuracy: 0.8882 - val_loss: 0.2718 - val_accuracy: 0.8880\n",
      "Epoch 2/10\n",
      "56538/56538 [==============================] - 1314s 23ms/step - loss: 0.2640 - accuracy: 0.8915 - val_loss: 0.2685 - val_accuracy: 0.8937\n",
      "Epoch 3/10\n",
      "56538/56538 [==============================] - 1310s 23ms/step - loss: 0.2544 - accuracy: 0.8935 - val_loss: 0.2880 - val_accuracy: 0.8915\n",
      "Epoch 4/10\n",
      "56538/56538 [==============================] - 1308s 23ms/step - loss: 0.2478 - accuracy: 0.8974 - val_loss: 0.2565 - val_accuracy: 0.8952\n",
      "Epoch 5/10\n",
      "56538/56538 [==============================] - 1312s 23ms/step - loss: 0.2400 - accuracy: 0.9000 - val_loss: 0.2652 - val_accuracy: 0.8966\n",
      "Epoch 6/10\n",
      "56538/56538 [==============================] - 1315s 23ms/step - loss: 0.2340 - accuracy: 0.9023 - val_loss: 0.2599 - val_accuracy: 0.8997\n",
      "Epoch 7/10\n",
      "56538/56538 [==============================] - 1316s 23ms/step - loss: 0.2273 - accuracy: 0.9062 - val_loss: 0.2572 - val_accuracy: 0.9018\n",
      "Epoch 8/10\n",
      "56538/56538 [==============================] - 1311s 23ms/step - loss: 0.2227 - accuracy: 0.9081 - val_loss: 0.2510 - val_accuracy: 0.9028\n",
      "Epoch 9/10\n",
      "56538/56538 [==============================] - 1300s 23ms/step - loss: 0.2173 - accuracy: 0.9097 - val_loss: 0.2552 - val_accuracy: 0.9058\n",
      "Epoch 10/10\n",
      "56538/56538 [==============================] - 1290s 23ms/step - loss: 0.2104 - accuracy: 0.9119 - val_loss: 0.2585 - val_accuracy: 0.9058\n",
      "CPU times: user 5h 9min 32s, sys: 19min 36s, total: 5h 29min 8s\n",
      "Wall time: 3h 38min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "file_path = \"lstm_model.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, 5, activation = 'relu', input_shape = (32,32)))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Conv1D(32, 5, activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 5))\n",
    "model.add(Conv1D(32,5, activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size = 32, epochs = 1, validation_data=(X_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedded_sequences = embedding_layer(sequence_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "x_train_transposed = copy.deepcopy(X_train)\n",
    "x_test_transposed = copy.deepcopy(X_test)\n",
    "\n",
    "y_train_T = copy.deepcopy(y_train)\n",
    "y_test_T = copy.deepcopy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transposed = x_train_transposed.T\n",
    "x_test_transposed = x_test_transposed.T\n",
    "\n",
    "y_train_T = y_train_T.T\n",
    "y_test_T = y_test_T.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def define_model():\n",
    "model_T = Sequential()\n",
    "model_T.add(Conv1D(64,3, activation = 'relu', input_shape = (56538, 1)))\n",
    "model_T.add(MaxPooling1D(pool_size = 3, strides = 2))\n",
    "model_T.add(Conv1D(32, 3, activation = 'relu'))\n",
    "model_T.add(MaxPooling1D(pool_size = 3, strides = 2))\n",
    "model_T.add(Flatten())\n",
    "model_T.add(Dense(100, activation = 'relu'))\n",
    "model_T.add(Dropout(0.3))\n",
    "model_T.add(Dense(4, activation = 'sigmoid'))\n",
    "\n",
    "model_T.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model_T.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_T.fit(x=x_train_transposed, y=y_train_T, batch_size = 32, epochs = 1, validation_data=(x_test_transposed, y_test_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
