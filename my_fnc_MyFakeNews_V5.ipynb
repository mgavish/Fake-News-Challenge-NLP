{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources Used  \n",
    "**For MG eyes**\n",
    "\n",
    "- http://www.fakenewschallenge.org/\n",
    "- https://github.com/Cisco-Talos/fnc-1\n",
    "- https://tedboy.github.io/nlps/generated/word2vec.html\n",
    "- https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n",
    "- https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "- https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#vector\n",
    "- https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec  \n",
    "\n",
    "- https://www.nltk.org/howto/sentiment.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "pip install notebook --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import numpy; print(\"NumPy\", numpy.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.float_format = '{:, .2f}'.format\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import save, load\n",
    "from numpy import savez_compressed\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#from scipy.misc import comb, logsumexp\n",
    "from sklearn.manifold import TSNE #a tool to visualize high dimensional data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD # dimensionality reduction using truncated SVD (AKA LSA)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.collocations import *\n",
    "import string #python module\n",
    "import re # python regex module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # a sentance tokenizer\n",
    "nltk.download('gutenberg') # a text corpora and lexical resources\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# A First Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.listdir('fnc-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Alias csv file paths\n",
    "Train_Bodies = 'fnc-1/train_bodies.csv'\n",
    "Train_Stances = 'fnc-1/train_stances.csv'\n",
    "\n",
    "Test_Bodies = 'fnc-1/competition_test_bodies.csv'\n",
    "Test_Stances = 'fnc-1/competition_test_stances.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in bodies to pandas\n",
    "TrainBodies_df = pd.read_csv(Train_Bodies)\n",
    "# rename column\n",
    "TrainBodies_df.rename(columns = {'Body ID':'Body_ID'}, inplace=True)\n",
    "# inspect df\n",
    "print(TrainBodies_df.info())\n",
    "print()\n",
    "print(\"TrainBodies_df\")\n",
    "TrainBodies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check df for missing values\n",
    "TrainBodies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# read in Stances data\n",
    "TrainStances_df = pd.read_csv(Train_Stances)\n",
    "# rename column\n",
    "TrainStances_df.rename(columns={'Body ID':'Body_ID'}, inplace=True)\n",
    "# inspect df\n",
    "print(TrainStances_df.info())\n",
    "print()\n",
    "#print(f\"There are {TrainStances_df['Body_ID'].nunique()} unique Body_ID values in the Train Stances dataset\")\n",
    "print()\n",
    "print(\"TrainStances_df\")\n",
    "TrainStances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TrainStances_df.Stance.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique Headlines: %s\" % TrainStances_df.Headline.nunique())\n",
    "print(\"Number of unique Body_IDs: %s\" % TrainStances_df.Body_ID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TrainStances_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainStances_df['Body_ID'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Body_ID min: %s\" % TrainStances_df['Body_ID'].min())\n",
    "print(\"Body_ID min: %s\" %TrainStances_df['Body_ID'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TrainStances_df.Stance.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TestBodies_df = pd.read_csv(Test_Bodies)\n",
    "TestBodies_df.rename(columns = {'Body ID':'Body_ID'}, inplace=True)\n",
    "print(TestBodies_df.info())\n",
    "print()\n",
    "display(TestBodies_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TestStances_df = pd.read_csv(Test_Stances)\n",
    "TestStances_df.rename(columns = {'Body ID':'Body_ID'}, inplace=True)\n",
    "print(TestStances_df.info())\n",
    "print()\n",
    "display(TestStances_df.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# is there anything to note about distribution of body_ID? \n",
    "# instantiate a figure and axes object\n",
    "fig, ax = plt.subplots()\n",
    "x = TrainStances_df['Body_ID']\n",
    "ax.hist(x, density=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are provided with three csv files, one of which is redundant.  \n",
    "In the other two, we have Train_Bodies which is 1,683 unique article bodies and their associated ID number.  In the second file labeled Train_Stances, there are 49,972 total observations consisting of 1,648 unique Headlines' with the 1,683 unique Body_IDs'.  This makes sense given the first part of the challenge is to classify the Headline and Body as Related or Unrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    " - punctuation\n",
    " - lowercase all\n",
    " - tokenize\n",
    " - remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join TrainBodies_df and TrainStances_df\n",
    "\n",
    "df_train = TrainStances_df.merge(TrainBodies_df, how = 'left', on = 'Body_ID', validate= 'm:1')\n",
    "print(df_train.shape)\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join TestBodies_df with TestStances_df\n",
    "df_test = TestStances_df.merge(TestBodies_df, how = 'left', on = 'Body_ID', validate= 'm:1')\n",
    "print(df_test.shape)\n",
    "df_test.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack train and test sets\n",
    "objs = [df_train, df_test]\n",
    "data = pd.concat(objs, axis = 0, join='outer')\n",
    "print(data.shape)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace target lables with numeric target values\n",
    "\n",
    "df_1 = copy.deepcopy(data)\n",
    "df_1.Stance.replace({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3}, inplace=True)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all text\n",
    "df_2 = copy.deepcopy(df_1)\n",
    "df_2['Headline'] = df_2['Headline'].str.lower()\n",
    "df_2['articleBody'] = df_2['articleBody'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and tokenize words\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = RegexpTokenizer (r\"(?u)\\b\\w\\w+\\b\")\n",
    "df_2['Headline_tokens'] = df_2['Headline'].map(tokenizer.tokenize)\n",
    "df_2['articleBody_tokens'] = df_2['articleBody'].map(tokenizer.tokenize)\n",
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### might want to keep one or more punctuation values in another notebook iteration, eg, $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``',\"_\"]\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['Headline_tokens'] = df_2['Headline_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "df_2['articleBody_tokens'] = df_2['articleBody_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias stemmer method\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "df_2['Headline_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.Headline_tokens], axis=1)\n",
    "df_2['articleBody_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.articleBody_tokens], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Count Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate grams and terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/Cisco-Talos/fnc-1/blob/master/tree_model/ngram.py\n",
    "\n",
    "def getUnigram(words):\n",
    "    #assert type(words) == []\n",
    "    return words\n",
    "\n",
    "def getBigram(words, join_string, skip=0):\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1, skip+2):\n",
    "                if i + k < L:\n",
    "                    lst.append(join_string.join([words[i], words[i+k]]))\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "        return lst\n",
    "                    \n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    #assert type(words) == []\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1, skip+2):\n",
    "                for k2 in range(1, skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append(join_string.join([words[i], words[i+k1], words[i+k1+k2]]))\n",
    "        return lst\n",
    "    else:\n",
    "        #set as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "        return lst\n",
    "    \n",
    "def getFourgram(words, join_string):\n",
    "\n",
    "    #assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 3:\n",
    "        lst = []\n",
    "        for i in xrange(L-3):\n",
    "            lst.append( join_string.join([words[i], words[i+1], words[i+2], words[i+3]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getTrigram(words, join_string)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def getBiterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny', 'boy']\n",
    "        Output: a list of biterm, e.g., ['I_am', 'I_Denny', 'I_boy', 'am_Denny', 'am_boy', 'Denny_boy']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1,L):\n",
    "                lst.append( join_string.join([words[i], words[j]]) )\n",
    "        return lst\n",
    "    \n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "    return lst\n",
    "    \n",
    "def getTriterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of triterm, e.g., ['I_am_Denny', 'I_Denny_am', 'am_I_Denny',\n",
    "        'am_Denny_I', 'Denny_I_am', 'Denny_am_I']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in xrange(L-2):\n",
    "            for j in xrange(i+1,L-1):\n",
    "                for k in xrange(j+1,L):\n",
    "                    lst.append( join_string.join([words[i], words[j], words[k]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as biterm\n",
    "        lst = getBiterm(words, join_string)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate unigram\n",
    "df_2[\"Headline_unigram\"] = df_2[\"Headline_tokens\"].map(lambda x: getUnigram(x))\n",
    "df_2[\"articleBody_unigram\"] = df_2[\"articleBody_tokens\"].map(lambda x: getUnigram(x))\n",
    "\n",
    "# generate bigram\n",
    "join_str = \"_\"\n",
    "df_2[\"Headline_bigram\"] = df_2[\"Headline_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "df_2[\"articleBody_bigram\"] = df_2[\"articleBody_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "        \n",
    "# generate trigram\n",
    "join_str = \"_\"\n",
    "df_2[\"Headline_trigram\"] = df_2[\"Headline_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "df_2[\"articleBody_trigram\"] = df_2[\"articleBody_unigram\"].map(lambda x: getTrigram(x, join_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# generate basic counting features\n",
    "\n",
    "'''\n",
    "def try_divide(x, y, val=0.0):\n",
    "    \"\"\" \n",
    "        Try to divide two numbers\n",
    "    \"\"\"\n",
    "    if y != 0.0:\n",
    "        val = float(x) / y\n",
    "    return val\n",
    "'''\n",
    "# calc percent of text corpus that is unique ( unique grams / ttl grams)\n",
    "\n",
    "grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "feat_names = [\"Headline\", \"articleBody\"]\n",
    "\n",
    "for feat_name in feat_names:\n",
    "    for gram in grams:\n",
    "        df_2[\"count_of_%s_%s\" % (feat_name, gram)] = list(df_2.apply(lambda x: len(x[feat_name + \"_\" + gram]), axis=1))\n",
    "        df_2[\"count_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "              list(df_2.apply(lambda x: len(set(x[feat_name + \"_\" + gram])), axis=1))\n",
    "        df_2[\"ratio_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "            df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)] / df_2[\"count_of_%s_%s\"%(feat_name,gram)]\n",
    "            #map(try_divide, df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)], df_2[\"count_of_%s_%s\"%(feat_name,gram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlapping n-grams count\n",
    "\n",
    "for gram in grams:\n",
    "    # find grams in each Headline n-gram that are also inside its coresponding articleBody n-gram\n",
    "    df_2[\"count_of_Headline_%s_in_articleBody\" % gram] = \\\n",
    "        list(df_2.apply(lambda x: sum([1. for w in x[\"Headline_\" + gram] if w in set(x[\"articleBody_\" + gram])]), axis=1))\n",
    "    \n",
    "    # return the ratio of overlapping grams to total grams\n",
    "    df_2[\"ratio_of_Headline_%s_in_articleBody\" % gram] = \\\n",
    "        df_2[\"count_of_Headline_%s_in_articleBody\" % gram] / df_2[\"count_of_Headline_%s\" % gram]\n",
    "        #map(try_divide, df[\"count_of_Headline_%s_in_articleBody\" % gram], df[\"count_of_Headline_%s\" % gram])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sentences in headline and body\n",
    "for feat_name in feat_names:\n",
    "    df_2['len_sent_%s' % feat_name] = df_2[feat_name].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names_bcf = [ n for n in df_2.columns \\\n",
    "                if \"count\" in n \\\n",
    "                or \"ratio\" in n \\\n",
    "                or \"len_sent\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names_bcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',50)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert basic count features to numpy array\n",
    "\n",
    "basic_count_feats = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = data[~data['target'].isnull()]\n",
    "#print ('train:')\n",
    "#print (train[['Headline_unigram','Body ID', 'count_of_Headline_unigram']])\n",
    "xBasicCountsTrain = df_2[feat_names].values\n",
    "outfilename_bcf_train = \"train.basic.pkl\"\n",
    "with open(outfilename_bcf_train, \"wb\") as outfile:\n",
    "    pickle.dump(feat_names, outfile, -1)\n",
    "    pickle.dump(xBasicCountsTrain, outfile, -1)\n",
    "print ('basic counting features for training saved in %s' % outfilename_bcf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_text(x):\n",
    "    res = '%s %s' % (' '.join(x['Headline_unigram']), ' '.join(x['articleBody_unigram']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate Headline and Body so we can fit a tfidf vectorizer that will learn the combined vocabulary\n",
    "\n",
    "df_2['all_text'] = list(df_2.apply(cat_text, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for row in df_2['Headline']:\n",
    "    for item in row.split():\n",
    "        new_list.append(item)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_words = []\n",
    "for row in df_2.all_text:\n",
    "    for item in row.split():\n",
    "        ttl_words.append(item)\n",
    "len(ttl_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_words = []\n",
    "for row in df_2.all_text:\n",
    "    for item in row.split():\n",
    "        ttl_words.append(item)\n",
    "set_ttl_words = set(ttl_words)\n",
    "len(set_ttl_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['all_text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of words in all_text\n",
    "\n",
    "#helper = copy.deepcopy(df_2['all_text'])\n",
    "count = df_2['all_text'].str.split().apply(len).value_counts()\n",
    "count.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a TfidfVectorizer on the concatenated strings (fit learns the vocabulary and idf)\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range = (1, 3), max_df= 0.8, min_df= 2)\n",
    "vec.fit(df_2['all_text'])\n",
    "vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fit and transform Headline using the learned vocabulary on the combined Headline + body corpus\n",
    "\n",
    "vecH = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df= 2, vocabulary=vocabulary)\n",
    "xHeadlineTfidf = vecH.fit_transform(df_2['Headline_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (xHeadlineTfidf.shape)\n",
    "\n",
    "\n",
    "outfilename_htfidf_train = \"MG-train.headline.tfidf.pkl\"\n",
    "with open (outfilename_htfidf_train, 'wb') as outfile:\n",
    "    pickle.dump(xHeadlineTfidf, outfile, -1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform articleBody using the learned vocabulary on the combined Headline + body corpus\n",
    "\n",
    "vecB = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)\n",
    "xBodyTfidf = vecB.fit_transform(df_2['articleBody_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (xBodyTfidf.shape)\n",
    "\n",
    "outfilename_btfidf_train = \"MG-train.body.tfidf.pkl\"\n",
    "with open(outfilename_btfidf_train, \"wb\") as outfile:\n",
    "    pickle.dump(xBodyTfidf, outfile, -1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    try:\n",
    "        if type(x) is np.ndarray: x = x.reshape(1, -1)\n",
    "        if type(y) is np.ndarray: y = y.reshape(1, -1)\n",
    "        d = cosine_similarity(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print (x)\n",
    "        print (y)\n",
    "        d = 0.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity between Headline and articleBody\n",
    "\n",
    "#load_xHeadlineTfidf = pickle.load(open(\"train.headline.tfidf.pkl\", 'rb'))\n",
    "#load_bodyTfidf = pickle.load(open(\"train.body.tfidf.pkl\", 'rb'))\n",
    "\n",
    "#simTfidf_train = cosine_similarity(xHeadlineTfidf, xBodyTfidf)\n",
    "simTfidf_train = np.asarray(list(map(cosine_sim, xHeadlineTfidf, xBodyTfidf)))[:, np.newaxis]\n",
    "\n",
    "print(simTfidf_train.shape)\n",
    "\n",
    "outfilename_simtfidf_train = \"MG-train.sim.tfidf.pkl\"\n",
    "with open(outfilename_simtfidf_train, \"wb\") as outfile:\n",
    "    pickle.dump(simTfidf_train, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Singular Value Decomposition (SVD) to the tf-idf features to reduce dimensionality and find latent topics.  \n",
    "Take tf-idf features and apply SVD.  THen take cosine similarities between the SVD features of Headline and articleBody.  This similarity metric is very telling of whether the body and headline are related or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xHeadlineTfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xHeadlineTfidf.shape)\n",
    "print(type(xHeadlineTfidf))\n",
    "print()\n",
    "print(xBodyTfidf.shape)\n",
    "print(type(xBodyTfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "xHBTfidf = vstack((xHeadlineTfidf, xBodyTfidf)).toarray() # toarray() converts the csr_matrix objects to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xHBTfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xHBTfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.fit(xHBTfidf) # fit to the combined train-test set (or the full training set for cv process)\n",
    "print ('xHeadlineTfidf.shape:')\n",
    "print (xHeadlineTfidf.shape)\n",
    "\n",
    "xHeadlineSvd = svd.transform(xHeadlineTfidf)\n",
    "print ('xHeadlineSvd.shape:')\n",
    "print (xHeadlineSvd.shape)\n",
    "\n",
    "xHeadlineSvdTrain = xHeadlineSvd\n",
    "outfilename_hsvd_train = \"train.headline.svd.pkl\"\n",
    "with open(outfilename_hsvd_train, \"wb\") as outfile:\n",
    "    pickle.dump(xHeadlineSvdTrain, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xBodySvd = svd.transform(xBodyTfidf)\n",
    "print ('xBodySvd.shape:')\n",
    "print (xBodySvd.shape)\n",
    "\n",
    "xBodySvdTrain = xBodySvd\n",
    "outfilename_bsvd_train = \"train.body.svd.pkl\"\n",
    "with open(outfilename_bsvd_train, \"wb\") as outfile:\n",
    "    pickle.dump(xBodySvdTrain, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_svd_train = cosine_similarity(xHeadlineSvd, xBodySvd)\n",
    "simSvd_train = np.asarray(list(map(cosine_sim, xHeadlineSvd, xBodySvd)))[:, np.newaxis]\n",
    "print ('sim_svd_train shape:')\n",
    "print (simSvd_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilename_simsvd_train = \"train.sim.svd.pkl\"\n",
    "with open(outfilename_simsvd_train, \"wb\") as outfile:\n",
    "    pickle.dump(simSvd_train, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using some talos code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['Headline_unigram_vec'] = df_2['Headline_tokens']\n",
    "df_2['articleBody_unigram_vec'] = df_2['articleBody_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nibabel\n",
    "#nibabel.load('GoogleNews-vectors-negative300.bin.gz').get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Headline_unigram_array = df_2['Headline_unigram_vec'].values\n",
    "print(\"df_2 Headline_unigram_vec type: %s\" % type(df_2['Headline_unigram_vec']))\n",
    "print(\"df_2 Headline_unigram_array type: %s\" % type('Headline_unigram_array'))\n",
    "print()\n",
    "\n",
    "headlineVec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*300), Headline_unigram_array)))\n",
    "headlineVec_norm = normalize(headlineVec)\n",
    "print(\"headline vec type: %s\" % type(headlineVec))\n",
    "print(\"headline vec shape:\" +  str(headlineVec.shape))\n",
    "print()\n",
    "print(\"headlineVec_norm vec type: %s\" % type(headlineVec_norm))\n",
    "print(\"headlineVec_norm vec shape:\" + str(headlineVec_norm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineVecTrain = headlineVec_norm\n",
    "outfilename_hvec_train = \"train.headline.word2vec.pkl\"\n",
    "with open(outfilename_hvec_train, \"wb\") as outfile:\n",
    "    pickle.dump(headlineVecTrain, outfile, -1)\n",
    "print ('headline word2vec features of training set saved in %s' % outfilename_hvec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Body_unigram_array = df_2['articleBody_unigram_vec'].values\n",
    "print(\"df_2 articleBody_unigram_vec type: %s\" % type(df_2['articleBody_unigram_vec']))\n",
    "print(\"df_2 Body_unigram_array type: %s\" % type('Body_unigram_array'))\n",
    "print()\n",
    "\n",
    "BodyVec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*300), Body_unigram_array)))\n",
    "#bodyVec = np.array(bodyVec)\n",
    "BodyVec_norm = normalize(BodyVec)\n",
    "\n",
    "print(\"BodyVec type: %s\" % type(BodyVec))\n",
    "print(\"BodyVec shape:\" +  str(BodyVec.shape))\n",
    "print()\n",
    "print(\"bodyVec_norm type: %s\" % type(BodyVec_norm))\n",
    "print(\"bodyVec_norm shape:\" + str(BodyVec_norm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train dataset\n",
    "bodyVecTrain = BodyVec_norm\n",
    "outfilename_bvec_train = \"train.body.word2vec.pkl\"\n",
    "with open(outfilename_bvec_train, \"wb\") as outfile:\n",
    "    pickle.dump(bodyVecTrain, outfile, -1)\n",
    "print ('body word2vec features of training set saved in %s' % outfilename_bvec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity between headline/body word2vec features\n",
    "simVec_w2v = np.asarray(list(map(cosine_sim, headlineVec_norm, BodyVec_norm)))[:, np.newaxis]\n",
    "print(type(simVec_w2v))\n",
    "print(simVec_w2v.shape)\n",
    "print(\"simVec_w2v num dimensions:\" + str(simVec_w2v.ndim))\n",
    "print(simVec_w2v[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simVecTrain = simVec_w2v\n",
    "outfilename_simvec_train = \"train.sim.word2vec.pkl\"\n",
    "with open(outfilename_simvec_train, \"wb\") as outfile:\n",
    "    pickle.dump(simVecTrain, outfile, -1)\n",
    "print ('word2vec sim. features of training set saved in %s' % outfilename_simvec_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Features\n",
    "\n",
    "- Use [NLTK Sentiment Analyzer](https://www.nltk.org/_modules/nltk/sentiment/vader.html) with [VADERSentiment](https://github.com/mgavish/vaderSentiment) to assign a sentiment polarity score to Headline and articelBody separately.\n",
    "- negative score means a negative opinion.\n",
    "- Do headline and articleBody have same sentiment?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate polarity score of each sentance in a Headline observation and return the average\n",
    "\n",
    "sid = SentimentIntensityAnalyzer() # https://www.nltk.org/howto/sentiment.html\n",
    "\n",
    "def compute_sentiment(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence) # https://www.nltk.org/howto/sentiment.html\n",
    "        result.append(ss)\n",
    "    return pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['headline_sentmts'] = df_2['Headline'].apply(lambda x: sent_tokenize(x)) # nltk's method sent_tokenize()\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.concat([df_2, df_2['headline_sentmts'].apply(lambda x: compute_sentiment(x))], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.rename(columns={'compound':'h_compound', 'neg':'h_neg', 'neu':'h_neu', 'pos':'h_pos'}, inplace=True)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineSenti = df_2[['h_compound','h_neg','h_neu','h_pos']].values\n",
    "print ('headlineSenti.shape:' + str(headlineSenti.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineSentiTrain = headlineSenti\n",
    "outfilename_hsenti_train = \"train.headline.senti.pkl\"\n",
    "with open(outfilename_hsenti_train, \"wb\") as outfile:\n",
    "    pickle.dump(headlineSentiTrain, outfile, -1)\n",
    "print ('headline sentiment features of training set saved in %s' % outfilename_hsenti_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['body_sents'] = df_2['articleBody'].map(lambda x: sent_tokenize(x))\n",
    "df_2 = pd.concat([df_2, df_2['body_sents'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
    "df_2.rename(columns={'compound':'b_compound', 'neg':'b_neg', 'neu':'b_neu', 'pos':'b_pos'}, inplace=True)\n",
    "bodySenti = df_2[['b_compound','b_neg','b_neu','b_pos']].values\n",
    "print ('bodySenti.shape:' + str(bodySenti.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_2.columns)\n",
    "cols.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodySentiTrain = bodySenti\n",
    "outfilename_bsenti_train = \"train.body.senti.pkl\"\n",
    "with open(outfilename_bsenti_train, \"wb\") as outfile:\n",
    "    pickle.dump(bodySentiTrain, outfile, -1)\n",
    "print ('body sentiment features of training set saved in %s' % outfilename_bsenti_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',100)\n",
    "# df_2.to_csv('df_2_afterAllFeatureGeneration.csv')\n",
    "df_2 = pd.read_csv('df_2_afterAllFeatureGeneration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15463387/pickle-putting-more-than-1-object-in-a-file/15463472\n",
    "\n",
    "# load features from pkl files\n",
    "\n",
    "# basic count features\n",
    "with open('train.basic.pkl', 'rb') as infile:\n",
    "    feat_names = pickle.load(infile)\n",
    "    xBasicCountsTrain = pickle.load(infile)\n",
    "'''\n",
    "# tfidf vectorized headline\n",
    "with open('MG-train.headline.tfidf.pkl', 'rb') as tfidf_head:\n",
    "    headline_tfidf = pickle.load(tfidf_head)\n",
    "    headline_tfidf = headline_tfidf.toarray()\n",
    "    \n",
    "# tfidf vectorized body\n",
    "with open('MG-train.body.tfidf.pkl', 'rb') as tfidf_in:\n",
    "    body_tfidf = pickle.load(tfidf_in)\n",
    "    body_tfidf = body_tfidf.toarray()\n",
    "''' \n",
    "# cosine similarity between tfidf headline and body\n",
    "with open('MG-train.sim.tfidf.pkl', 'rb') as tfidf_sim:\n",
    "    sim_tfidf = pickle.load(tfidf_sim)\n",
    "\n",
    "# svd of headline\n",
    "with open('train.headline.svd.pkl', 'rb') as svd_head:\n",
    "    headline_svd = pickle.load(svd_head)\n",
    "    \n",
    "# svd of body\n",
    "with open('train.body.svd.pkl', 'rb') as svd_body:\n",
    "    body_svd = pickle.load(svd_body)\n",
    "\n",
    "# svd of tfidf cosine similarity\n",
    "with open('train.sim.svd.pkl', 'rb') as svd_sim:\n",
    "    sim_svd = pickle.load(svd_sim)\n",
    "    \n",
    "# w2v headline\n",
    "with open('train.headline.word2vec.pkl', 'rb') as w2v_head:\n",
    "    headline_w2v = pickle.load(w2v_head)\n",
    "    \n",
    "# w2v body\n",
    "with open('train.body.word2vec.pkl', 'rb') as w2v_body:\n",
    "    body_w2v = pickle.load(w2v_body)\n",
    "    \n",
    "# headlinen sentiment scores\n",
    "with open('train.headline.senti.pkl', 'rb') as senti_head:\n",
    "    headline_senti = pickle.load(senti_head)\n",
    "    \n",
    "# body sentiment scores\n",
    "with open('train.body.senti.pkl', 'rb') as senti_body:\n",
    "    body_senti = pickle.load(senti_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train.basic.pkl: ' + str(os.path.getsize('train.basic.pkl')))\n",
    "print('MG-train.headline.tfidf.pkl: ' + str(os.path.getsize('MG-train.headline.tfidf.pkl')))\n",
    "print('MG-train.body.tfidf.pkl: ' + str(os.path.getsize('MG-train.body.tfidf.pkl')))\n",
    "print('MG-train.sim.tfidf.pkl: ' + str(os.path.getsize('MG-train.sim.tfidf.pkl')))\n",
    "print('train.body.svd.pkl: ' + str(os.path.getsize('train.body.svd.pkl')))\n",
    "print('train.sim.svd.pkl: ' + str(os.path.getsize('train.sim.svd.pkl')))\n",
    "print('train.headline.word2vec.pkl: ' + str(os.path.getsize('train.headline.word2vec.pkl')))\n",
    "print('train.body.word2vec.pkl: ' + str(os.path.getsize('train.body.word2vec.pkl')))\n",
    "print('train.headline.senti.pkl: ' + str(os.path.getsize('train.headline.senti.pkl')))\n",
    "print('train.body.senti.pkl: ' + str(os.path.getsize('train.body.senti.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features into numpy array\n",
    "\n",
    "#arrays = [xBasicCountsTrain, headline_tfidf, body_tfidf, sim_tfidf, headline_svd, body_svd, sim_svd, headline_w2v, body_w2v, headline_senti,body_senti]\n",
    "\n",
    "arrays = [xBasicCountsTrain,  sim_tfidf, headline_svd, body_svd, sim_svd, headline_w2v, body_w2v, headline_senti,body_senti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for array in arrays:\n",
    "    print(array.ndim)\n",
    "    print(array.shape)\n",
    "    print(type(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_data = np.hstack(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_model_data.pkl', 'wb') as all_data:\n",
    "    pickle.dump(model_data, all_data, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.savez_compressed('model_data.npz', model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import features and target data\n",
    "\n",
    "target_y = copy.deepcopy(df_1['Stance']).to_numpy().reshape(-1,1)\n",
    "target_y.shape\n",
    "np.savez_compressed('model_target_data.npz', target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75385, 1)\n",
      "(75385, 836)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from numpy import load\n",
    "target_y = load('model_target_data.npz')\n",
    "target_y = target_y['arr_0']\n",
    "print(target_y.shape)\n",
    "features_x =  load('model_data.npz')\n",
    "features_x = features_x['arr_0']\n",
    "print(features_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_x[0:60300]\n",
    "y_train = target_y[0:60300]\n",
    "\n",
    "X_test = features_x[60300:75386]\n",
    "y_test = target_y[60300:75386]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "## Score using scorer.py (provided in https://github.com/FakeNewsChallenge/fnc-1) on TEST set\n",
    "#from scorer import score_submission, print_confusion_matrix, score_defaults, SCORE_REPORT\n",
    "from score import report_score, LABELS, score_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [LABELS[int(a)] for a in boost_clf.predict(X_test)]\n",
    "actual = [LABELS[int(a)] for a in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fc624107e09f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_fold_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_score\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_fold_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score for fold \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" was - \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fold' is not defined"
     ]
    }
   ],
   "source": [
    "fold_score, _ = score_submission(actual, predicted)\n",
    "max_fold_score, _ = score_submission(actual, actual)\n",
    "score = fold_score/max_fold_score\n",
    "print(\"Score for fold \"+ str(fold) + \" was - \" + str(score))\n",
    "if score > best_score:\n",
    "    best_score = score\n",
    "    best_fold = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# instantiate XGBoost classifier\n",
    "boost_clf = xgb.XGBClassifier()\n",
    "## use stratefied kfold for classification task\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1)\n",
    "\n",
    "#boost_scores = cross_val_score(boost_clf, features_x, target_y, scoring='roc_auc', cv=kfold)\n",
    "boost_scores = cross_val_predict(boost_clf, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchcv\n",
    "params_xgb = {\n",
    "\n",
    "    'max_depth': [6],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'subsample': [1.0],\n",
    "    'eta': [0.1],\n",
    "    'silent': [1],\n",
    "    #'objective': 'multi:softmax',\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data = copy.deepcopy(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_in = open('train.basic.pkl', 'rb')\n",
    "basic_count_features = pickle.load(bcf_in)\n",
    "basic_count_features = [basic_count_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(basic_count_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basic_count_features[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Predictions\n",
    "\n",
    "1D CNN on Headline and articleBody (at word level).  Output of CNN sent to MLP with 4 class outputs (agree, disagree, discuss, unrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras import backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Conv1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
